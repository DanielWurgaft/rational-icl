algorithmic_solutions:
- memorized
- generalized
attention_dropout: 0.0
block_size: 321
context_length: 320
setting: categorical-sequence
eval_steps: 500
gradient_accumulation_steps: 1
hidden_size: 64
inputs_key: input_ids
intermediate_size: 32
labels_key: next_token_distribution
learning_rate: 0.0005
logging_steps: 500
lr_scheduler_type: constant_with_warmup
max_grad_norm: 1
max_position_embeddings: 321
max_steps: 100000
metric_name: KL Divergence
mlp_expansion_factor: 0.5
model_type: gpt-neo-x
name_suffix: ''
num_attention_heads: 1
num_dims: 8
num_eval_tasks: 500
num_hidden_layers: 1
num_key_value_heads: 1
num_tasks: 8
per_device_eval_batch_size: 64
per_device_train_batch_size: 64
prior_params:
- 1
- 1
- 1
- 1
- 1
- 1
- 1
- 1
random_seed: 1
save_steps:
- 20
- 58
- 115
- 193
- 291
- 408
- 545
- 703
- 879
- 1076
- 1293
- 1529
- 1785
- 2062
- 2357
- 2673
- 3009
- 3364
- 3739
- 4135
- 4549
- 4984
- 5439
- 5913
- 6407
- 6921
- 7455
- 8009
- 8583
- 9176
- 9789
- 10422
- 11075
- 11748
- 12441
- 13153
- 13885
- 14637
- 15409
- 16201
- 17013
- 17844
- 18695
- 19566
- 20457
- 21368
- 22298
- 23249
- 24219
- 25209
- 26219
- 27249
- 28298
- 29368
- 30457
- 31566
- 32695
- 33844
- 35012
- 36201
- 37409
- 38637
- 39885
- 41153
- 42440
- 43747
- 45075
- 46422
- 47789
- 49175
- 50582
- 52008
- 53455
- 54921
- 56407
- 57912
- 59438
- 60983
- 62548
- 64133
- 65738
- 67363
- 69008
- 70672
- 72356
- 74060
- 75784
- 77528
- 79291
- 81075
- 82878
- 84701
- 86544
- 88407
- 90289
- 92192
- 94114
- 96056
- 98018
- 100000
start_token: s
tokenizer_vocab:
  '0': 0
  '1': 1
  '2': 2
  '3': 3
  '4': 4
  '5': 5
  '6': 6
  '7': 7
  s: 8
warmup_steps: 5000
weight_decay: 0.0
