algorithmic_solutions:
- memorized
- generalized
attention_dropout: 0.0
block_size: 129
context_length: 128
setting: categorical-sequence
eval_steps: 500
gradient_accumulation_steps: 1
hidden_size: 64
inputs_key: input_ids
intermediate_size: 256
labels_key: next_token_distribution
learning_rate: 0.0005
logging_steps: 500
lr_scheduler_kwargs: {}
lr_scheduler_type: inverse_sqrt
max_grad_norm: 1
max_position_embeddings: 129
max_steps: 10000000
metric_name: KL Divergence
mlp_expansion_factor: 4
model_type: gpt-neo-x
name_suffix: lr_annealing
num_attention_heads: 1
num_dims: 8
num_eval_tasks: 500
num_hidden_layers: 1
num_key_value_heads: 1
num_tasks: 512
per_device_eval_batch_size: 64
per_device_train_batch_size: 64
prior_params:
- 1
- 1
- 1
- 1
- 1
- 1
- 1
- 1
random_seed: 1
save_steps:
- 20
- 413
- 1311
- 2712
- 4616
- 7024
- 9936
- 13351
- 17270
- 21693
- 26619
- 32049
- 37983
- 44420
- 51360
- 58805
- 66753
- 75204
- 84159
- 93618
- 103580
- 114046
- 125016
- 136489
- 148466
- 160946
- 173930
- 187418
- 201409
- 215904
- 230902
- 246404
- 262410
- 278919
- 295932
- 313448
- 331468
- 349992
- 369020
- 388550
- 408585
- 429123
- 450165
- 471710
- 493759
- 516312
- 539368
- 562928
- 586991
- 611558
- 636629
- 662203
- 688281
- 714863
- 741948
- 769536
- 797629
- 826224
- 855324
- 884927
- 915034
- 945644
- 976758
- 1008376
- 1040497
- 1073122
- 1106250
- 1139882
- 1174018
- 1208657
- 1243800
- 1279446
- 1315596
- 1352250
- 1389407
- 1427068
- 1465233
- 1503901
- 1543073
- 1582748
- 1622927
- 1663609
- 1704796
- 1746485
- 1788679
- 1831376
- 1874576
- 1918281
- 1962489
- 2007200
- 2052415
- 2098134
- 2144356
- 2191082
- 2238311
- 2286044
- 2334281
- 2383021
- 2432265
- 2482013
- 2532264
- 2583019
- 2634277
- 2686039
- 2738305
- 2791074
- 2844347
- 2898123
- 2952403
- 3007187
- 3062474
- 3118265
- 3174560
- 3231358
- 3288659
- 3346465
- 3404774
- 3463586
- 3522902
- 3582722
- 3643045
- 3703872
- 3765203
- 3827037
- 3889375
- 3952216
- 4015561
- 4079410
- 4143762
- 4208618
- 4273978
- 4339841
- 4406207
- 4473078
- 4540452
- 4608329
- 4676710
- 4745595
- 4814983
- 4884875
- 4955271
- 5026170
- 5097573
- 5169479
- 5241889
- 5314803
- 5388220
- 5462141
- 5536565
- 5611493
- 5686925
- 5762860
- 5839299
- 5916242
- 5993688
- 6071638
- 6150091
- 6229048
- 6308508
- 6388473
- 6468940
- 6549912
- 6631387
- 6713365
- 6795847
- 6878833
- 6962323
- 7046316
- 7130812
- 7215813
- 7301317
- 7387324
- 7473835
- 7560850
- 7648368
- 7736390
- 7824916
- 7913945
- 8003478
- 8093514
- 8184054
- 8275098
- 8366645
- 8458696
- 8551250
- 8644308
- 8737870
- 8831935
- 8926504
- 9021576
- 9117152
- 9213232
- 9309815
- 9406902
- 9504493
- 9602587
- 9701185
- 9800286
- 9899891
- 10000000
start_token: s
tokenizer_vocab:
  '0': 0
  '1': 1
  '2': 2
  '3': 3
  '4': 4
  '5': 5
  '6': 6
  '7': 7
  s: 8
warmup_steps: 500
weight_decay: 0.0
zipf_param: 0
